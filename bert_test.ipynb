{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "\n",
    "from mmbt.data.helpers import get_data_loaders\n",
    "from mmbt.models import get_model\n",
    "from mmbt.utils.logger import create_logger\n",
    "from mmbt.utils.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b460b0adae64494a96358c336ffe347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019b837602c642628bf8fe5abfc377c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c72b218d8a45bf8bcf9cf6100c9ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', \"'\", 's', 'going', 'on', '?']\n",
      "[2054, 1005, 1055, 2183, 2006, 1029]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer インスタンスの生成\n",
    "# 対象モデルは'bert-base-uncased'\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer関数の動作確認\n",
    "tokens = tokenizer.tokenize(\"What's going on?\")\n",
    "print(tokens)\n",
    "\n",
    "# convert_tokens_to_ids関数の動作確認\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "# BERT固有の特殊トークン達\n",
    "cls_token = tokenizer.cls_token\n",
    "sep_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "print(cls_token, sep_token, pad_token, unk_token)\n",
    "\n",
    "# idによるトークン表記\n",
    "cls_token_idx = tokenizer.cls_token_id\n",
    "sep_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "print(cls_token_idx, sep_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力テキストのトークン化関数\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    # 252までで切る\n",
    "    tokens = tokens[:254-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'japanese', 'stock', 'market', 'experienced', 'a', 'notable', 'ups', '##wing', 'in', 'the', 'first', 'week', 'of', '202', '##3', ',', 'reflecting', 'a', 'positive', 'sentiment', 'among', 'investors', 'and', 'financial', 'analysts', '.', 'this', 'bull', '##ish', 'trend', 'can', 'be', 'attributed', 'to', 'a', 'combination', 'of', 'factors', 'that', 'made', 'japanese', 'e', '##qui', '##ties', 'particularly', 'appealing', '.', 'during', 'the', 'early', 'part', 'of', '202', '##3', ',', 'the', 'japanese', 'stock', 'market', 'was', 'dominated', 'by', 'bull', '##ish', 'sentiment', ',', 'partly', 'due', 'to', 'the', 'country', \"'\", 's', 'continued', 'negative', 'interest', 'rates', '.', 'this', 'contrasted', 'with', 'the', 'trend', 'in', 'other', 'g', '##7', 'countries', ',', 'where', 'interest', 'rates', 'were', 'raised', 'to', 'combat', 'inflation', '.', 'the', 'nik', '##kei', '-', '225', 'index', ',', 'a', 'key', 'indicator', 'of', 'the', 'japanese', 'stock', 'market', ',', 'grew', 'by', '30', '%', 'in', 'the', 'first', 'half', 'of', 'the', 'year', '.', 'this', 'growth', 'was', 'supported', 'by', 'a', 'balance', 'of', 'supply', 'and', 'demand', ',', 'as', 'evidenced', 'by', 'the', 'formation', 'of', 'a', 'range', 'framing', 'the', 'index', '’', 's', 'fluctuations', 'in', 'the', 'second', 'half', 'of', 'the', 'year', '.', 'however', ',', 'there', 'was', 'growing', 'speculation', 'that', 'the', 'bank', 'of', 'japan', 'might', 'begin', 'raising', 'interest', 'rates', 'after', 'years', 'of', 'being', 'stuck', 'in', 'negative', 'territory', '.', 'this', 'speculation', 'was', 'fueled', 'by', 'expectations', 'that', 'the', 'us', ',', 'europe', ',', 'and', 'other', 'regions', 'might', 'be', 'nearing', 'the', 'peak', 'of', 'their', 'interest', 'rate', 'hike', '##s', '.', 'the', 'possibility', 'of', 'a', 'shift', 'in', 'japan', \"'\", 's', 'monetary', 'policy', 'led', 'to', 'a', 'growing', 'bear', '##ish', 'sentiment', 'by', 'the', 'end', 'of', 'november', ',', 'with', 'the', 'nik', '##kei', '225', 'experiencing', 'a', 'drop', 'of', 'almost', '5', '%', '.', 'despite', 'these']\n",
      "['the', 'us', 'stock', 'market', 'sentiment', 'in', 'the', 'first', 'week', 'of', 'january', '202', '##3', 'showed', 'a', 'cautiously', 'optimistic', 'outlook', ',', 'influenced', 'by', 'various', 'factors', 'including', 'sector', 'performances', ',', 'rate', 'hike', 'expectations', ',', 'and', 'company', '-', 'specific', 'news', '.', '1', '.', '*', '*', 'january', 'indicator', 'tri', '##fect', '##a', 'and', 'sector', 'performance', '*', '*', ':', 'the', 'stock', 'market', 'experienced', 'what', 'is', 'known', 'as', 'the', '\"', 'january', 'indicator', 'tri', '##fect', '##a', '.', '\"', 'this', 'refers', 'to', 'a', 'santa', 'claus', 'rally', ',', 'positive', 'first', 'five', 'days', 'of', 'january', ',', 'and', 'a', 'positive', 'january', 'bar', '##ometer', '.', 'the', 'occurrence', 'of', 'all', 'three', 'indicators', 'historically', 'suggests', 'a', 'favorable', 'market', 'in', 'the', 'following', '11', 'months', '.', 'in', 'terms', 'of', 'sector', 'performance', ',', 'consumer', 'discretion', '##ary', 'and', 'communication', 'services', 'led', 'the', 'gains', '.', 'the', 'nas', '##da', '##q', 'composite', 'showed', 'a', 'strong', 'performance', ',', 'especially', 'in', 'technology', 'stocks', ',', 'while', 'small', '-', 'cap', 'stocks', 'indicated', 'by', 'the', 's', '&', 'p', '600', 'small', 'cap', 'index', 'also', 'rose', 'significantly', '.', '2', '.', '*', '*', 'federal', 'reserve', 'and', 'rate', 'hike', 'odds', '*', '*', ':', 'the', 'market', 'was', 'anticipating', 'a', '25', 'basis', 'point', 'rate', 'increase', 'at', 'the', 'february', 'federal', 'reserve', 'meeting', '.', 'this', 'expectation', 'was', 'reflected', 'in', 'the', 'pricing', 'of', 'fed', 'fund', 'futures', '.', 'treasury', 'yields', 'saw', 'some', 'weakness', ',', 'with', 'the', '10', '-', 'year', 'treasury', 'yield', 'dropping', 'to', '3', '.', '51', '%', ',', 'which', 'was', 'below', 'the', 'october', 'peak', 'of', '4', '.', '25', '%', '.', '3', '.', '*', '*', 'corporate', 'earnings', 'and', 'stock', 'performance', '*', '*', ':', 'about', 'one', '-', 'third', 'of', 's', '&', 'p']\n",
      "[101, 1996, 2149, 4518, 3006, 15792, 1999, 1996, 2034, 2733, 1997, 2254, 16798, 2509, 3662, 1037, 15151, 21931, 17680, 1010, 5105, 2011, 2536, 5876, 2164, 4753, 4616, 1010, 3446, 21857, 10908, 1010, 1998, 2194, 1011, 3563, 2739, 1012, 1015, 1012, 1008, 1008, 2254, 17245, 13012, 25969, 2050, 1998, 4753, 2836, 1008, 1008, 1024, 1996, 4518, 3006, 5281, 2054, 2003, 2124, 2004, 1996, 1000, 2254, 17245, 13012, 25969, 2050, 1012, 1000, 2023, 5218, 2000, 1037, 4203, 19118, 8320, 1010, 3893, 2034, 2274, 2420, 1997, 2254, 1010, 1998, 1037, 3893, 2254, 3347, 18721, 1012, 1996, 14404, 1997, 2035, 2093, 20390, 7145, 6083, 1037, 11119, 3006, 1999, 1996, 2206, 2340, 2706, 1012, 1999, 3408, 1997, 4753, 2836, 1010, 7325, 19258, 5649, 1998, 4807, 2578, 2419, 1996, 12154, 1012, 1996, 17235, 2850, 4160, 12490, 3662, 1037, 2844, 2836, 1010, 2926, 1999, 2974, 15768, 1010, 2096, 2235, 1011, 6178, 15768, 5393, 2011, 1996, 1055, 1004, 1052, 5174, 2235, 6178, 5950, 2036, 3123, 6022, 1012, 1016, 1012, 1008, 1008, 2976, 3914, 1998, 3446, 21857, 10238, 1008, 1008, 1024, 1996, 3006, 2001, 26481, 1037, 2423, 3978, 2391, 3446, 3623, 2012, 1996, 2337, 2976, 3914, 3116, 1012, 2023, 17626, 2001, 7686, 1999, 1996, 20874, 1997, 7349, 4636, 17795, 1012, 9837, 16189, 2387, 2070, 11251, 1010, 2007, 1996, 2184, 1011, 2095, 9837, 10750, 7510, 2000, 1017, 1012, 4868, 1003, 1010, 2029, 2001, 2917, 1996, 2255, 4672, 1997, 1018, 1012, 2423, 1003, 1012, 1017, 1012, 1008, 1008, 5971, 16565, 1998, 4518, 2836, 1008, 1008, 1024, 2055, 2028, 1011, 2353, 1997, 1055, 1004, 1052, 3156, 3316, 2988, 1037, 1019, 1003, 6689, 1999, 1053, 2549, 11372, 1010, 4102, 2000, 2019, 3517, 1017, 1012, 1016, 1003, 6689, 1012, 2750, 2023, 1010, 2045, 2020, 11105, 2066, 2943, 1010, 3919, 2015, 1010, 1998, 7325, 19258, 5649, 2008, 2387, 3278, 16565, 3930, 1012, 5546, 1010, 1996, 5409, 1011, 4488, 15768, 1997, 16798, 2475, 2387, 2019, 2779, 3623, 1997, 2322, 1012, 1015, 1003, 1999, 2220, 16798, 2509, 1010, 9104, 1037, 2460, 1011, 2744, 7065, 2545, 3258, 1997, 15849, 11614, 15768, 2738, 2084, 1037, 8050, 5670, 1999, 3006, 4105, 1012, 1018, 1012, 1008, 1008, 3747, 1997, 2194, 1011, 3563, 2739, 1008, 1008, 1024, 3265, 3316, 2036, 5105, 3006, 15792, 1012, 2005, 6013, 1010, 26060, 1005, 1055, 6661, 2253, 2039, 2044, 13856, 3976, 7659, 1999, 2859, 1010, 2096, 2793, 7198, 1004, 3458, 1005, 1055, 6661, 6430, 6022, 2349, 2000, 10528, 16852, 1012, 3465, 3597, 1005, 1055, 4518, 4227, 2044, 7316, 3893, 2285, 4341, 2951, 1012, 1019, 1012, 1008, 1008, 3452, 3006, 10949, 1008, 1008, 1024, 1996, 2034, 2733, 1997, 2254, 16798, 2509, 2701, 3020, 2005, 2149, 15768, 1010, 22464, 2011, 1037, 11119, 5841, 3189, 1998, 5971, 2739, 1012, 1996, 17324, 8913, 5285, 10450, 18605, 5950, 1006, 6819, 2595, 1007, 1010, 2411, 5240, 2004, 1037, 3571, 7633, 1010, 10548, 2011, 2340, 1003, 1999, 2254, 1010, 8131, 1037, 9885, 1999, 3006, 5285, 10450, 18605, 1012, 1999, 12654, 1010, 1996, 2034, 2733, 1997, 2254, 16798, 2509, 1999, 1996, 2149, 4518, 3006, 2001, 4417, 2011, 1037, 4666, 1997, 27451, 5533, 2011, 4753, 4616, 1998, 17145, 15792, 2349, 2000, 3171, 20390, 1998, 5971, 16565, 1012, 2096, 2045, 2001, 1037, 3893, 17680, 2241, 2006, 1996, 2254, 20390, 1010, 1996, 3006, 2815, 7591, 2000, 3446, 21857, 2015, 1998, 3265, 5971, 4616, 1012, 7604, 1024, 1011, 4518, 7507, 21217, 1012, 4012, 100, 1020, 1526, 3120, 100, 1011, 17235, 2850, 4160, 100, 1021, 1526, 3120, 100, 1011, 20643, 5446, 100, 1022, 1526, 3120, 100, 100, 1023, 1526, 3120, 100, 102]\n",
      "tensor([[  101,  1996,  2887,  4518,  3006,  5281,  1037,  3862, 11139,  9328,\n",
      "          1999,  1996,  2034,  2733,  1997, 16798,  2509,  1010, 10842,  1037,\n",
      "          3893, 15792,  2426,  9387,  1998,  3361, 18288,  1012,  2023,  7087,\n",
      "          4509,  9874,  2064,  2022,  7108,  2000,  1037,  5257,  1997,  5876,\n",
      "          2008,  2081,  2887,  1041, 15549,  7368,  3391, 16004,  1012,  2076,\n",
      "          1996,  2220,  2112,  1997, 16798,  2509,  1010,  1996,  2887,  4518,\n",
      "          3006,  2001,  6817,  2011,  7087,  4509, 15792,  1010,  6576,  2349,\n",
      "          2000,  1996,  2406,  1005,  1055,  2506,  4997,  3037,  6165,  1012,\n",
      "          2023, 22085,  2007,  1996,  9874,  1999,  2060,  1043,  2581,  3032,\n",
      "          1010,  2073,  3037,  6165,  2020,  2992,  2000,  4337, 14200,  1012,\n",
      "          1996, 23205, 29501,  1011, 14993,  5950,  1010,  1037,  3145, 17245,\n",
      "          1997,  1996,  2887,  4518,  3006,  1010,  3473,  2011,  2382,  1003,\n",
      "          1999,  1996,  2034,  2431,  1997,  1996,  2095,  1012,  2023,  3930,\n",
      "          2001,  3569,  2011,  1037,  5703,  1997,  4425,  1998,  5157,  1010,\n",
      "          2004, 21328,  2011,  1996,  4195,  1997,  1037,  2846, 20241,  1996,\n",
      "          5950,  1521,  1055, 28892,  1999,  1996,  2117,  2431,  1997,  1996,\n",
      "          2095,  1012,  2174,  1010,  2045,  2001,  3652, 12143,  2008,  1996,\n",
      "          2924,  1997,  2900,  2453,  4088,  6274,  3037,  6165,  2044,  2086,\n",
      "          1997,  2108,  5881,  1999,  4997,  3700,  1012,  2023, 12143,  2001,\n",
      "         17999,  2011, 10908,  2008,  1996,  2149,  1010,  2885,  1010,  1998,\n",
      "          2060,  4655,  2453,  2022, 23454,  1996,  4672,  1997,  2037,  3037,\n",
      "          3446, 21857,  2015,  1012,  1996,  6061,  1997,  1037,  5670,  1999,\n",
      "          2900,  1005,  1055, 12194,  3343,  2419,  2000,  1037,  3652,  4562,\n",
      "          4509, 15792,  2011,  1996,  2203,  1997,  2281,  1010,  2007,  1996,\n",
      "         23205, 29501, 14993, 13417,  1037,  4530,  1997,  2471,  1019,  1003,\n",
      "          1012,  2750,  2122,  5936,  1010,  2070,  1997,  1996,  2088,  1005,\n",
      "          1055,  2087,  8228,  9387,  1998,  2350,  2813,  2395,  5085,  5224,\n",
      "          1037,  3893, 17680,  2006,  1996,  2887,  4518,  3006,  1012,  2027,\n",
      "          2387,  2062, 14961,  4022,  1010,  2130,  2004,  1996,  5041,  2327,\n",
      "          7646,  5950,  2584,  2049,  3284,  2504,  2144,  2901,  1012,  2023,\n",
      "         27451,  2001,  6576,  2349,  2000,  1996,  2709,  1997, 14200,  1010,\n",
      "          9229, 18668,  5651,  1010,  1998,  2019, 20380,  2011,  4069,  9387,\n",
      "          2066,  6031, 28305,  2102,  1012,  2009,  1005,  1055,  2590,  2000,\n",
      "          3602,  2008,  1996,  2925,  3257,  1997,  1996,  2887,  4518,  3006,\n",
      "          2052,  3497, 12530,  2006,  2195,  5876,  1010,  2164,  1996,  2924,\n",
      "          1997,  2900,  1005,  1055, 12194,  3343,  6567,  1010,  3795,  3171,\n",
      "         12878,  1010,  1998, 14316, 15792,  1012,  1996,  2220, 16798,  2509,\n",
      "          7087,  4509,  9874,  1999,  1996,  2887,  3006,  7645,  1996,  3006,\n",
      "          1005,  1055, 24501, 18622, 10127,  1998,  2049,  8702,  2791,  2000,\n",
      "          3795,  9387,  1010,  2750,  1996, 23430,  6061,  1997,  3037,  3446,\n",
      "          3431,  1012,  2005,  2062,  6851, 20062,  1998,  4106,  1010,  2017,\n",
      "          2064,  6523,  2000,  1996,  2434,  4216,  1024,  1996,  2900,  2335,\n",
      "          1998, 23292, 26915,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([1, 500])\n"
     ]
    }
   ],
   "source": [
    "text_2023_01_jp = \"The Japanese stock market experienced a notable upswing in the first week of 2023, reflecting a positive sentiment among investors and financial analysts. This bullish trend can be attributed to a combination of factors that made Japanese equities particularly appealing. \\\n",
    "During the early part of 2023, the Japanese stock market was dominated by bullish sentiment, partly due to the country's continued negative interest rates. This contrasted with the trend in other G7 countries, where interest rates were raised to combat inflation. The Nikkei-225 index, a key indicator of the Japanese stock market, grew by 30% in the first half of the year. This growth was supported by a balance of supply and demand, as evidenced by the formation of a range framing the index’s fluctuations in the second half of the year. \\\n",
    "However, there was growing speculation that the Bank of Japan might begin raising interest rates after years of being stuck in negative territory. This speculation was fueled by expectations that the US, Europe, and other regions might be nearing the peak of their interest rate hikes. The possibility of a shift in Japan's monetary policy led to a growing bearish sentiment by the end of November, with the Nikkei 225 experiencing a drop of almost 5%. \\\n",
    "Despite these concerns, some of the world's most renowned investors and major Wall Street banks maintained a positive outlook on the Japanese stock market. They saw more upside potential, even as the broad Topix index reached its highest level since 1990. This optimism was partly due to the return of inflation, improving shareholder returns, and an endorsement by prominent investors like Warren Buffett. \\\n",
    "It's important to note that the future direction of the Japanese stock market would likely depend on several factors, including the Bank of Japan's monetary policy decisions, global economic trends, and investor sentiment. The early 2023 bullish trend in the Japanese market demonstrated the market's resilience and its attractiveness to global investors, despite the looming possibility of interest rate changes. \\\n",
    "For more detailed insights and analysis, you can refer to the original sources: The Japan Times and FXOpen. \"\n",
    "\n",
    "text_2023_01_us = \"\"\"\n",
    "The US stock market sentiment in the first week of January 2023 showed a cautiously optimistic outlook, influenced by various factors including sector performances, rate hike expectations, and company-specific news.\n",
    "\n",
    "1. **January Indicator Trifecta and Sector Performance**: The stock market experienced what is known as the \"January Indicator Trifecta.\" This refers to a Santa Claus rally, positive first five days of January, and a positive January Barometer. The occurrence of all three indicators historically suggests a favorable market in the following 11 months. In terms of sector performance, Consumer Discretionary and Communication Services led the gains. The Nasdaq Composite showed a strong performance, especially in technology stocks, while small-cap stocks indicated by the S&P 600 Small Cap index also rose significantly.\n",
    "\n",
    "2. **Federal Reserve and Rate Hike Odds**: The market was anticipating a 25 basis point rate increase at the February Federal Reserve meeting. This expectation was reflected in the pricing of fed fund futures. Treasury yields saw some weakness, with the 10-year Treasury yield dropping to 3.51%, which was below the October peak of 4.25%.\n",
    "\n",
    "3. **Corporate Earnings and Stock Performance**: About one-third of S&P 500 companies reported a 5% decline in Q4 profits, compared to an expected 3.2% decline. Despite this, there were sectors like Energy, Industrials, and Consumer Discretionary that saw significant earnings growth. Notably, the worst-performing stocks of 2022 saw an average increase of 20.1% in early 2023, suggesting a short-term reversion of oversold stocks rather than a fundamental shift in market leadership.\n",
    "\n",
    "4. **Influence of Company-Specific News**: Individual companies also influenced market sentiment. For instance, Tesla's shares went up after announcing price cuts in China, while Bed Bath & Beyond's shares declined significantly due to bankruptcy considerations. Costco's stock gained after reporting positive December sales data.\n",
    "\n",
    "5. **Overall Market Dynamics**: The first week of January 2023 closed higher for US stocks, spurred by a favorable jobs report and corporate news. The CBOE Volatility Index (VIX), often regarded as a fear gauge, decreased by 11% in January, indicating a decrease in market volatility.\n",
    "\n",
    "In summary, the first week of January 2023 in the US stock market was marked by a mix of optimism driven by sector performances and cautious sentiment due to economic indicators and corporate earnings. While there was a positive outlook based on the January indicators, the market remained sensitive to rate hikes and individual corporate performances.\n",
    "\n",
    "References: \n",
    "- StockCharts.com【6†source】\n",
    "- Nasdaq【7†source】\n",
    "- Yahoo Finance【8†source】【9†source】\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenize(text_2023_01_jp)\n",
    "print(tokens)\n",
    "tokens = tokenize(text_2023_01_us)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(text_2023_01_us)\n",
    "print(input_ids)\n",
    "encoding = tokenizer(\n",
    "    text_2023_01_jp, \n",
    "    max_length =500, \n",
    "    padding =\"max_length\", \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(encoding.input_ids)\n",
    "print(encoding.input_ids.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "embedded torch.Size([1, 768])\n",
      "tensor([[0.1933, 0.2893]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# GPU利用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# 事前学習済みモデルのロード\n",
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# モデルの定義\n",
    "# 事前学習済みモデルの後段に線形関数を追加し、この出力で感情分析をする\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.out = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        #embedded = [batch size, emb dim]\n",
    "        embedded = self.bert(text)[1]\n",
    "        print(\"embedded\" , embedded.size() )\n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "        output = self.out(embedded)\n",
    "        \n",
    "        return output\n",
    "# モデルインスタンスの生成\n",
    "# 出力は感情分析なので2\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = BERTSentiment(bert, OUTPUT_DIM).to(device)\n",
    "model.eval()\n",
    "\n",
    "input = encoding.input_ids.to(device)\n",
    "predictions = model(input)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 学習データのデータ構造定義\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# torchtextのバージョンアップに伴い、legacyを付ける必要あり\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m      6\u001b[0m \u001b[39m# 入力データ\u001b[39;00m\n\u001b[1;32m      7\u001b[0m TEXT \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(batch_first \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                   use_vocab \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                   \u001b[39m# 上で定義したトークン化関数\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                   eos_token \u001b[39m=\u001b[39m sep_token_idx,\n\u001b[1;32m     15\u001b[0m                   pad_token \u001b[39m=\u001b[39m pad_token_idx)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "# 学習データのデータ構造定義\n",
    "\n",
    "# torchtextのバージョンアップに伴い、legacyを付ける必要あり\n",
    "from torchtext.legacy import data\n",
    "\n",
    "# 入力データ\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  # 上で定義したトークン化関数\n",
    "                  tokenize = tokenize,\n",
    "                  # 前処理として各トークンをIDに変換\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = cls_token_idx,\n",
    "                  eos_token = sep_token_idx,\n",
    "                  pad_token = pad_token_idx)\n",
    " \n",
    "# 正解ラベル\n",
    "LABEL = data.LabelField()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
